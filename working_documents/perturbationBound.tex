\documentclass[11pt]{article}
% Include statements
\usepackage{graphicx}
\usepackage{amsfonts,amssymb,amsmath,amsthm}
\usepackage[numbers,square]{natbib}
\usepackage[left=1in,top=1in,right=1in,bottom=1in,nohead]{geometry}
\usepackage[all]{xy}
\usepackage{multirow,rotating,array}
\usepackage[ruled,lined]{algorithm2e}
\SetKw{KwSet}{Set}
%\usepackage{algorithm,algorithmic}
\usepackage{pdfsync}
\usepackage{setspace}
\usepackage{bbm}
\usepackage{moresize}
\usepackage{hyperref}
\hypersetup{backref,colorlinks=true,citecolor=blue,linkcolor=blue,urlcolor=blue}
\renewcommand{\qedsymbol}{$\blacksquare$}

% Bibliography
\bibliographystyle{plainnat}

% Theorem environments
\usepackage{aliascnt}

\newtheorem{theorem}{Theorem}[section]

\newaliascnt{result}{theorem}
\newtheorem{result}[theorem]{Result}
\aliascntresetthe{result}
\providecommand*{\resultautorefname}{Result}
\newaliascnt{lemma}{theorem}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}
\providecommand*{\lemmaautorefname}{Lemma}
\newaliascnt{prop}{theorem}
\newtheorem{proposition}[prop]{Proposition}
\aliascntresetthe{prop}
\providecommand*{\propautorefname}{Proposition}
\newaliascnt{cor}{theorem}
\newtheorem{corollary}[cor]{Corollary}
\aliascntresetthe{cor}
\providecommand*{\corautorefname}{Corollary}
\newaliascnt{conj}{theorem}
\newtheorem{conjecture}[conj]{Conjecture}
\aliascntresetthe{conj}
\providecommand*{\conjautorefname}{Corollary}
\newaliascnt{def}{theorem}
\newtheorem{definition}[def]{Definition}
\aliascntresetthe{def}
\providecommand*{\defautorefname}{Definition}

\newtheorem{assumption}{Assumption}
\renewcommand{\theassumption}{\Alph{assumption}}
\providecommand*{\assumptionautorefname}{Assumption}

\def\algorithmautorefname{Algorithm}
\renewcommand*{\figureautorefname}{Figure}%
\renewcommand*{\tableautorefname}{Table}%
\renewcommand*{\partautorefname}{Part}%
\renewcommand*{\chapterautorefname}{Chapter}%
\renewcommand*{\sectionautorefname}{Section}%
\renewcommand*{\subsectionautorefname}{Section}%
\renewcommand*{\subsubsectionautorefname}{Section}% 


% Macros
\def\indep{\perp\!\!\!\perp}
\newcommand{\given}{\ \vert\ }
\newcommand{\F}{\mathcal{F}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Expect}[1]{\E\left[ #1 \right]}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\email}[1]{\href{mailto:#1}{#1}}
\newcommand{\X}{\mathbb{X}}
\renewcommand{\S}{\mathcal{S}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\trace}{tr}


\begin{document}
\noindent\textbf{\sc DW
        \hfill Possible proof technique
        \hfill \today}
\rule{6.5in}{1pt}
\section{Overview}
Suppose $A \in \R^{m\times n}$ and $\tilde{A} = A + E$.  Also, $U^{\top}AV = \begin{bmatrix} D \\ 0 \end{bmatrix}$ and $\tilde{U}^{\top}\tilde{A}\tilde{V} = \begin{bmatrix} \tilde{D} \\ 0 \end{bmatrix}$.
There is a main theorem for bounding the distance between $D$ and $\tilde{D}$:

\begin{theorem}[Mirsky]
\[
\norm{\tilde{D} - D} \leq \norm{E}
\]
where the norm can be any unitarily equivalent norm (e.g. $\norm{\cdot}_2$ or $\norm{\cdot}_F$).
\end{theorem}

\section{Our purposes}
Let the covariance matrix for $X$ be
\[
\Sigma 
= 
 \begin{bmatrix} 
 \Sigma_{1} & 0  \\ 0 & \Sigma_2 
 \end{bmatrix}
\]
where
\[
\Sigma_1 
= 
 \begin{bmatrix} 
 \Sigma_{11} & 0  \\ 0 & \Sigma_{12} 
 \end{bmatrix} \in \R^{p_1 \times p_1}
\]
and $\Sigma_{1s} \in \R^{p_{1s} \times p_{1s}}$ for $s = 1,2$.  Let $\S = \{1,\ldots,p_1\}$ and $\S^c = \{1,\ldots,p\} \setminus \S$.  Lastly, we define $\Sigma_\S = [\Sigma_1, 0]^{\top}$
to be the first $p_1$ columns of $\Sigma$.

The goal here is to extend the results of the SPCA paper by including the possibility in $\Sigma_1$ that we have missed some important features  (hence their $\Sigma_1$ corresponds 
to our $\Sigma_{11}$.  The model for $X$ is then (here I'm writing/thinking about a single latent factor model, I'm presuming that complexifying that to multi-factor will be a matter
of notation):
\[
X_{ij} = v_i \rho_j + \sigma z_{ij}
\]
where $v_i,z_{ij}$ are all mutually independent standard normals and $\rho_j \neq 0$ iff $j \in \S$.

\subsection{The result}
As $ F = \X^{\top}\X_1$, then\footnote{We will need to divide through by some function of $n$.}
\begin{align}
F & = \left[\sum_{i=1}^n(v_i\rho_j + \sigma z_{ij})(v_i\rho_k + \sigma z_{ik})\right]_{1\leq j\leq p, k \in \S} \\
& = 
\begin{bmatrix}
\sum_{i=1}^n(v_i\rho_j + \sigma z_{ij})(v_i\rho_k + \sigma z_{ik})\\
\sum_{i=1}^n(\sigma z_{ij})(v_i\rho_k + \sigma z_{ik})
\end{bmatrix} \\
& 
=
\begin{bmatrix}
\sum_{i=1}^n(v_i^2\rho_j\rho_k + \sigma z_{ij}v_i\rho_k + \sigma z_{ik}v_i\rho_j +  \sigma^2 z_{ij}z_{ik})\\
\sum_{i=1}^n( \sigma z_{ij}v_i\rho_k +  \sigma^2 z_{ij}z_{ik})
\end{bmatrix},
\end{align}
where the top block has $j \in S$ and the bottom block has $j \in \S^c$ (this convention will persist for the rest of the document).

Using the result from the previous theorem, write $A = \Sigma_\S$ and $E = F - \Sigma_\S$.  Hence, the singular values of $\Sigma_\S$  and $F$ will be close if $F - \Sigma_\S$ is small.

Note the expectation of $F$:
\[
\E F =
\begin{bmatrix}
n \rho\rho^{\top} + n\sigma^2 I \\
0
\end{bmatrix}.
\]
Hence, 
\[
E = 
\begin{bmatrix}
F - \E F
\end{bmatrix}.
-
\begin{bmatrix}
n \rho\rho^{\top} + n\sigma^2 I \\
0
\end{bmatrix}.
\]
So, up to the $n\sigma^2I$ factor, we have to bound the norm difference between a random matrix and it's expectation.
\begin{align}
F - \E F -
 \begin{bmatrix}
n \rho\rho^{\top} + n\sigma^2 I \\
0
\end{bmatrix}
 & =
\begin{bmatrix}
\sum_{i=1}^nv_i^2\rho_j\rho_k  - n \rho_j\rho_k\\
0
\end{bmatrix}
+
\begin{bmatrix}
\sigma \sum_{i=1}^n v_i(z_{ij}\rho_k + z_{ik}\rho_j) \\
\sigma \sum_{i=1}^n v_iz_{ij}\rho_k + \sigma^2 \sum_{i=1}^n z_{ij}z_{ik}
\end{bmatrix}
+ \\
& \qquad + 
\begin{bmatrix}
 \sigma^2 \sum_{i=1}^n z_{ij}z_{ik} - n\sigma^2 I \\
0
\end{bmatrix} \\
& = (i) + (ii) + (iii).
\end{align}
Now,
\[
(i) = 
(\sum_{i=1}^nv_i^2 -n)
\begin{bmatrix}
\rho\rho^{\top}\\
0
\end{bmatrix}
\]
and
\[
(iii) = 
\sigma^2 
\begin{bmatrix}
\sum_{i=1}^n Z_{i\S}Z_{i\S}^{\top} - n I \\
0
\end{bmatrix},
\]
where $Z_{i\S} = [z_{ij}]_{j \in \S}$. Similarly, $Z_{i\S^c} = [z_{ij}]_{j \in \S^c}$.  Hence, 
\[
(ii) = 
\sigma \sum_{i=1}^n v_i
\begin{bmatrix}
 Z_{i\S}\rho^{\top}\\
 Z_{i\S^c}\rho^{\top}
\end{bmatrix}
+
\sigma \sum_{i=1}^n v_i
\begin{bmatrix}
\rho Z_{i\S}^{\top}\\
0
\end{bmatrix}
+
\sigma^2 \sum_{i=1}^n 
\begin{bmatrix}
0 \\
Z_{i\S} Z_{i\S^c}^{\top}\\
\end{bmatrix}.
\]
Now, concentration results can be used to show $E$ is small in Frobenius norm.  Presumably, we can find some results about spectral norm as well (which would
probably be more useful as it would allow us to say $|d_j - \tilde{d_j}| \leq \norm{E}_2$ for all $j$).
\bibliography{../SPCA.bib}
\end{document}

